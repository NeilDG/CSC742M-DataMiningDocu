\section{Experiments and Results}
This section contains the experiments performed and the results of the models used. As mentioned in the previous section, we have the nominal type and the numeric type of dataset for both games. We proceed with results on using numeric values for \textbf{DAU-Day 7} followed by nominal values to represent ranges for \textbf{DAU-Day7}.

For the numeric type, we consider the following metrics for evaluation; correlation coefficient, mean absolute error (MAE), root mean squared error (RMSE), relative absolute error (RAE), and root relative squared error (RRSE). We discuss M5Base, REPTree and Multilayer Perceptron as separate subsections.

\subsection{Correlation Analysis}
We measured correlation of attributes with \textbf{DAU-Day7} to determine which attributes should we include for training different prediction models. This was our first basis for feature selection. However, we did not solely rely on correlation analysis and considered other feature selection techniques\footnote{Various search and selection techniques are available in WEKA for feature selection.} to improve the accuracy of the models.

We primarily used correlation analysis to determine if there are strong relationships with \textbf{DAU-Day7} for both games. We observed that JNC has significantly strong relationships while DNC did not.

\begin{table}[h]
\centering
\caption{Correlation of Attributes against \textbf{DAU-Day7}}
\label{table:correlation_table}
\begin{tabular}{|c|c|c|}
\hline 
 & Jungle Cubes & Dragon Cubes\\ 
\hline 
Cohort Size & 0.6802 & 0.4655\\ 
\hline 
Day1 & -0.0054 & -0.2016\\ 
\hline 
Sessions & \textbf{0.8241} & 0.4261\\ 
\hline 
ActiveUsers & \textbf{0.9037} & 0.4630\\ 
\hline 
LevelPlayedEvents & 0.6817& 0.4226\\ 
\hline 
LevelSuccessEvents & \textbf{0.7279} & 0.4700\\ 
\hline 
LevelFailedEvents & 0.6217 & 0.3111\\ 
\hline 
MKTExpenses & \textbf{0.7349} & 0.3747\\ 
\hline 
TotalPurchases &\textbf{ 0.8507} & 0.1577\\ 
\hline 
AverageRating & 0.0803 & 0.1578\\ 
\hline 
CrashesANRDay1 & 0.6194 & 0.2668\\ 
\hline 
AvgSessionSeconds & -0.0595 & -0.2073\\
\hline 
MedianSessionSeconds & 0.1644 & -0.0938\\  
\hline
\end{tabular}
\end{table} 

JNC attributes yielded significantly higher positive correlation against \textbf{DAU-Day7}. There are four strong correlation values (in bold text)\footnote{We use 0.7 as threshold for indicating strong relationship.} while DNC do not have any. This indicates further proof that DNC do not yield any noticeable patterns due to its fair performance in the gaming market. JNC, on the other hand, already have a trend if we're going to use correlation as a measure.

\subsection{Feature Selection}
In order for our learning algorithms chosen to perform best, we filtered out the unneeded attributes primarily influenced by using established feature selection techniques. We use a wrapper scheme for feature selection for learning algorithms to perform best by removing unnecessary features. This technique is discussed in \cite{ref:wrappers_feature_selection}. We used a best-first search method together with the selected attribute evaluator. For each learning algorithm used in our study, different set of features are selected.

Basing from the initial features "proposed" to be selected by using this scheme, we manually selected, or removed some features deemed significant by this technique. Solely relying on the wrapper scheme still induced noise on the final outcome of the model. We based our manual selection method through correlation analysis and repeated observations on how it affects the overall accuracy of the model.

\subsection{M5Base}
We attempt to use M5Base provided by WEKA which is based from \cite{ref:m5base} due to its combined nature to induce decision trees and output linear models at the leaves. This suits well for our study due to the numeric form of \textbf{DAU-Day7} and the ability to output a formula which can easily be used for marketing and business decision purposes.

Table \ref{table:m5base_features} shows the selected features used for creating the model using M5Base. Notice the differences of features selected for both games.
\begin{table}[h]
\centering
\caption{Selected features for M5Base}
\label{table:m5base_features}
\begin{tabular}{|c|c|c|}
\hline 
 & Jungle Cubes & Dragon Cubes\\ 
\hline 
Features & Cohort Size & CrashesANRDay1 
\\& Day 1 & LevelFailedEvents 
\\& LevelFailedEvents & LevelSuccessEvents
\\& LevelSuccessEvents & Sessions
\\& Sessions & MKTExpenses 
\\& MKTExpenses & AvgSessionSeconds 
\\& TotalPurchases & 
\\& MedianSessionSeconds &\\ 
\hline 

\end{tabular}
\end{table} 

Applying the said features from Table \ref{table:m5base_features}, we attempt to use m5Base as predictor using the default configuration and resulted in the following measures seen in Table \ref{table:m5base_results}. JNC has significantly higher magnitude of error, referring to MAE and RMSE measures compared to DNC. However, the RAE and RRSE for JNC is significantly lower than DNC which means that the model of JNC seems to be more reliable than a simple predictor\footnote{Simple predictor is just using the mean as predicted value for DAU-Day7.}. If we take a look into their corresponding test sets, notice the significant change of metrics for both games.

\begin{table*}[h]
\centering
\caption{m5Base Results for JNC and DNC}
\label{table:m5base_results}
\begin{tabular}{|c|c|c|c|c|}
\hline 
M5Base & JNC & DNC & JNC-Test & DNC-Test \\ 
\hline 
Correlation coefficient & 0.9499 & 0.7673 & 0.9442 & 0.3731
\\Mean absolute error & 363.3713 & 226.1379 & 380.6705 & 276.2044
\\Root mean squared error & 522.864 &  390.7313 & 410.3039 & 285.4435
\\Relative absolute error & 25.1493\% & 55.0067\% & 9.3048\% & 31.9779\%
\\Root relative squared error  & 31.0991\% & 64.3702\% & 9.9679\% & 32.966\% 
\\Total Number of Instances & 250 & 250 & 28 & 28
\\
\hline 
\end{tabular}
\end{table*} 

Figure \ref{fig:correl_m5base} shows the correlation coefficent differences. Notice that JNC has high correlation. DNC has high correlation on its training set but using the test set, the correlation has dropped significantly which makes the model questionable for real world use.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{figures/m5Base_1.png} 
\caption{Correlation coefficents for m5Base}
\label{fig:correl_m5base}
\end{figure}

Figure \ref{fig:error_m5base} shows the summary of error tendencies of both games using m5Base. We can see that the MAE and RMSE of JNC is somewhat higher than DNC, but looking into RAE and RRSE, we see that JNC outperforms DNC by a huge difference. The prediction model becomes more reliable than a simple predictor for JNC's case.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{figures/m5Base_2.png} 
\includegraphics[scale=0.4]{figures/m5Base_3.png} 
\caption{Error tendencies for m5Base}
\label{fig:error_m5base}
\end{figure}

\subsection{REPTree}
We applied REPTree induction for JNC and DNC as it builds a regression tree that is also suitable for real world use. Examining the results, it did not perform on par with the outcome of M5Base. However, the outcome of this learning algorithm  has comparable results than any other learning algorithms we choose. We therefore, mention the results of the model. Fewer features were deemed significant by our feature selection method in REPTree as seen in Table \ref{table:reptree_features}.

\begin{table}[h]
\centering
\caption{Selected features for REPTree}
\label{table:reptree_features}
\begin{tabular}{|c|c|c|}
\hline 
 & Jungle Cubes & Dragon Cubes\\ 
\hline 
Features & Cohort Size & CrashesANRDay1 
\\& DailyAverageRating & LevelFailedEvents 
\\& LevelFailedEvents & Sessions
\\& MKTExpenses & MKTExpenses
\\& MedianSessionSeconds &  
\\& MKTExpenses &  
\\ 
\hline 
\end{tabular}
\end{table}

\begin{table*}[h]
\centering
\caption{REPTree Results for JNC and DNC}
\label{table:reptree_results}
\begin{tabular}{|c|c|c|c|c|}
\hline 
M5Base & JNC & DNC & JNC-Test & DNC-Test \\ 
\hline 
Correlation coefficient & 0.934 & 0.6565 & 0.6198 & 0
\\Mean absolute error & 417.3018 & 262.0065 & 943.719 & 166.4048
\\Root mean squared error & 602.4758 &  456.0032 & 1030.8327 & 177.1683
\\Relative absolute error &28.8819\% & 63.7316\% & 23.0675\% & 19.2657\%
\\Root relative squared error  & 35.8343\% & 75.1233\% & 25.0429\% & 20.4612\% 
\\Total Number of Instances & 250 & 250 & 28 & 28
\\
\hline 
\end{tabular}
\end{table*}

Table \ref{table:reptree_results} shows the summary of metrics for JNC and DNC. Figure \ref{fig:correl_reptree} and Figure \ref{fig:error_reptree} shows the graph of JNC and DNC's correlation coefficients and error tendencies respectively. Notice that JNC and DNC's results on cross-validation procedure did not really differ from the results using m5Base. However, attempting to apply this to our provided test set, we observed that values vary greatly.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{figures/REPTree_1.png} 
\caption{Correlation coefficents for REPTree}
\label{fig:correl_reptree}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{figures/REPTree_2.png} 
\includegraphics[scale=0.4]{figures/REPTree_3.png} 
\caption{Error tendencies for REPTree}
\label{fig:error_reptree}
\end{figure}

Notice that on \textit{JNC-Test} tab, the error measures has significantly increased compared to m5Base. On the other hand, result in \textbf{DNC-Test} tab indicates that this learning algorithm faired better than m5Base when applied to practical use. However, we cannot fully recommend using REPTree for practical use since the correlation coefficient is zero. There is no real pattern between variables that affect \textbf{DAU-Day7} and may not fully predict \textbf{DAU-Day7} on various events.

\subsection{Multilayer Perceptron}
We attempted to use artificial neural network using Multilayer Perceptron as learning algorithm for predicting \textbf{DAU-Day7}; on the impression that this learning algorithm is flexible and has various parameters involved that we can continuously adjust to improve the accuracy of the model. In this subsection, we discuss briefly how we adjust the settings to generate the optimal network.

Table \ref{table:m5base_features} shows the selected features for multilayer perceptron using our method specified earlier in this paper.

\begin{table}[h]
\centering
\caption{Selected features for Multilayer Perceptron}
\label{table:mp_features}
\begin{tabular}{|c|c|c|}
\hline 
 & Jungle Cubes & Dragon Cubes\\ 
\hline 
Features & Cohort Size & Cohort Size 
\\& MKTExpenses & MKTExpenses 
\\& TotalPurchases &
\\& MedianSessionSeconds &
\\& AvgSessionSeconds &  
\\ 
\hline 
\end{tabular}
\end{table}

We found that the number of hidden layers appropriate for this experiment is the total number of attributes and classes of our dataset\footnote{Actual count from best results has 6 hidden layers. Tally is retrieved after selecting features}. We relied on acceptable training time to modify the learning rate and the total number of epochs. Using the default values provided did not yield acceptable results. Modifying these said parameters\footnote{Learning rate is set to 0.001, number of epochs set to 5000.} noticeably increased the accuracy of the model that is comparable for this study.

\begin{table*}[h]
\centering
\caption{Multilayer Perceptron Results for JNC and DNC}
\label{table:mp_results}
\begin{tabular}{|c|c|c|c|c|}
\hline 
Multilayer Perceptron & JNC & DNC & JNC-Test & DNC-Test \\ 
\hline 
Correlation coefficient & 0.9315 & 0.5197 & 0.0383 & 0.6587
\\Mean absolute error & 457.19 & 277.4295 & 1530.9904 & 480.7897
\\Root mean squared error & 608.1291 &  516.1081 & 1677.0443 & 483.6563
\\Relative absolute error &31.6426\% & 67.4831\% & 37.4223\% & 55.664\%
\\Root relative squared error  & 36.1706\% & 85.0251\% & 40.7419\% & 55.8576\% 
\\Total Number of Instances & 250 & 250 & 28 & 28
\\
\hline 
\end{tabular}
\end{table*}

Using cross-validation procedure, it can be observed that multilayer perceptron performed acceptably for JNC while DNC did not. The error measures for DNC for multilayer perceptron is the highest among the three numeric prediction models presented.

Applying multilayer perceptron on our test set, observe that JNC have a very low correlation coefficient this time, which is very different from the outcome of REPTree and m5Base. Notice that the MAE and RMSE have significantly increased which makes multilayer perceptron less than ideal for practical use in JNC.

Using multilayer perceptron for DNC, we can observe that the results from the test set closely resemble results from the cross-validation tests in m5Base. The RAE and RRSE is more than 50\% which may indicate that using multilayer perceptron is only halfway better than a simple predictor. 

Table \ref{table:mp_results} shows the summary of the metrics using multilayer perceptron. Figure \ref{fig:correl_mp} and Figure \ref{fig:error_mp} shows the graph for the correlation coefficient and error metrics respectively.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{figures/MP_1.png} 
\caption{Correlation coefficents for Multilayer Perceptron}
\label{fig:correl_mp}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{figures/MP_2.png} 
\includegraphics[scale=0.4]{figures/MP_3.png} 
\caption{Error tendencies for Multilayer Perceptron}
\label{fig:error_mp}
\end{figure}

